{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category = FutureWarning)\n",
    "warnings.filterwarnings('ignore', category = DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category = UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re \n",
    "from ast import literal_eval\n",
    "from random import seed\n",
    "from itertools import chain\n",
    "import multiprocessing\n",
    "from nltk.tokenize import word_tokenize,WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,CountVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "import string\n",
    "import stop_words\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.corpus.util\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'.\\Desktop\\Stream.csv',encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sound Bite Text', 'Sentiment'], dtype='object')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4525"
      ]
     },
     "execution_count": 706,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Length of Dataframe\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15742"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#No. of rows containing text data\n",
    "len(df[df[\"Sound Bite Text\"].isna() == False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rows not containing any text data\n",
    "len(df[df[\"Sound Bite Text\"].isna() == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post ID</th>\n",
       "      <th>Sound Bite Text</th>\n",
       "      <th>Ratings and Scores</th>\n",
       "      <th>Title</th>\n",
       "      <th>Source Type</th>\n",
       "      <th>Post Type</th>\n",
       "      <th>Media Type</th>\n",
       "      <th>URL</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Published</th>\n",
       "      <th>...</th>\n",
       "      <th>Quoted Author Handle</th>\n",
       "      <th>Total Engagements</th>\n",
       "      <th>Post Comments</th>\n",
       "      <th>Post Likes</th>\n",
       "      <th>Post Shares</th>\n",
       "      <th>Post Views</th>\n",
       "      <th>Post Dislikes</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Product Hierarchy</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Post ID, Sound Bite Text, Ratings and Scores, Title, Source Type, Post Type, Media Type, URL, Domain, Published, Date, (GMT-04:00), New, York, Author ID, Author Location - Country 1, Author Location - State/Province 1, Author Location - City 1, Author Location - Country 2, Author Location - State/Province 2, Author Location - City 2, Author Location - Other, No. of Followers/Daily Unique Visitors, Professions, Interests, Positive Objects, Negative Objects, Richness, Tags, Quoted Post, Quoted Author Name, Quoted Author Handle, Total Engagements, Post Comments, Post Likes, Post Shares, Post Views, Post Dislikes, Product Name, Product Hierarchy, Rating]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Sound Bite Text\"].isna() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rows not containing text data in \"Sound Bite Text\" column are dropped.\n",
    "df.drop(df[df[\"Sound Bite Text\"].isna() == True].index.values,axis = 0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df[df['Sentiment'].isna() == False].index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating seperate column for cleaned data(Without Hashtags)\n",
    "df[\"soundBites\"] = df[\"Sound Bite Text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Hashtags\n",
    "df[\"soundBites\"] = df[\"soundBites\"].apply(lambda row: re.sub(r'@[A-Za-z0-9_]+', '',row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['soundBites'] = df[\"soundBites\"].apply(lambda row: re.sub('[^a-zA-Z0-9#-_]',' ',row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['soundBites'] = df['soundBites'].apply(lambda row: re.sub('#\\w+',' ',row))\n",
    "df['soundBites'] = df['soundBites'].apply(lambda row: re.sub('[â❤]',' ',row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Non-words\n",
    "df['soundBites'] = df[\"soundBites\"].apply(lambda row: re.sub('[;+>\\/<,.)¿(&?:?%*=-]+','',row))\n",
    "#df[\"soundBites\"] = df[\"soundBites\"].apply(lambda row: re.sub('\\W+',\" \",row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"soundBites\"] = df[\"soundBites\"].apply(lambda row: re.sub('http\\S+','',row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Digits\n",
    "df[\"soundBites\"] = df[\"soundBites\"].apply(lambda row: re.sub('\\d+',\" \",row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['soundBites'] = df[\"soundBites\"].apply(lambda row: re.sub(r'\\w+(.)$',' ',row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['soundBites'] = df[\"soundBites\"].apply(lambda row: re.sub(r'\\s+[.]',' ',row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['soundBites'] = df['soundBites'].apply(lambda row: row.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [for, amarpatan, region, of, madhya, pradesh, ...\n",
       "1        [consumer, called, up, and, asked, about, if, ...\n",
       "2        [consumer, called, up, as, he, has, a, pack, o...\n",
       "3        [retailer, called, in, again, and, said, that,...\n",
       "4        [consumer, called, to, inquire, about, the, me...\n",
       "5        [consumer, called, up, as, he, wanted, to, kno...\n",
       "6        [consumer, called, us, and, shared, the, feedb...\n",
       "7        [consumer, called, in, as, she, is, using, hig...\n",
       "8        [source, nestlecom, , , , , firstname, sharad,...\n",
       "9        [customer, is, looking, for, a, technician, fo...\n",
       "10       [consumer, called, to, inquire, about, the, sh...\n",
       "11       [source, nestlecom, , , , , firstname, jay, , ...\n",
       "12       [consumer, is, looking, for, technician, for, ...\n",
       "13       [hi, gudevening, , iam, nishan, karkera, here,...\n",
       "14       [customer, called, up, as, he, want, distribut...\n",
       "15       [consumer, is, looking, for, a, technician, as...\n",
       "16       [consumer, called, saying, that, he, is, feedi...\n",
       "17       [dear, sir, , , , i, am, , ddurai, murugan, , ...\n",
       "18       [consumer, called, in, to, inquire, about, the...\n",
       "19       [consumer, concern, , consumer, called, and, i...\n",
       "20       [sub, resume, for, requirement, , , , dear, si...\n",
       "21       [we, are, not, receiving, the, supply, of, nes...\n",
       "22       [kindly, change, your, latest, maggi, tv, comm...\n",
       "23       [sub, regarding, getting, distributorship, or,...\n",
       "24       [customer, called, us, and, said, that, he, wa...\n",
       "25       [dear, sirmadam, , , , , came, to, know, about...\n",
       "26       [sub, availability, of, maggi, , , , dear, sir...\n",
       "27       [consumer, called, and, inquired, if, nestle, ...\n",
       "28                                      [unproductive, , ]\n",
       "29       [consumer, called, up, as, he, wanted, to, kno...\n",
       "                               ...                        \n",
       "15712    [, bought, this, today, at, chennai, and, choc...\n",
       "15713    [hi, nestle, team, , i, am, your, regular, cus...\n",
       "15714    [my, daughter, bought, me, a, jar, of, , nesca...\n",
       "15715    [consumer, concern, consumer, called, in, and,...\n",
       "15716    [consumer, concern, called, to, consumer, and,...\n",
       "15717                                                  [n]\n",
       "15718    [wat, the, hell, is, this, put, some, care, to...\n",
       "15719    [this, is, to, bring, to, your, notice, about,...\n",
       "15720    [, consumer, concern, consumer, called, up, an...\n",
       "15721    [hai, , , , please, find, the, attached, image...\n",
       "15722    [consumer, concern, retailer, called, and, he,...\n",
       "15723    [, ma, april, b, , , , w, , , cb, , , april, ,...\n",
       "15724    [dear, nestle, india, , , , , i, purchased, a,...\n",
       "15725    [consumer's, concern, consumer, called, in, wi...\n",
       "15726    [dear, nestle, team, , , , one, of, our, custo...\n",
       "15727    [dear, , , , , , , , , , , we, are, a, regular...\n",
       "15728    [consumer, concern, consumer, complained, for,...\n",
       "15729    [we, have, received, and, delivered, , a, full...\n",
       "15730    [consumer, concern, consumer, called, in, and,...\n",
       "15731    [hi, , , , , , recently, i, purchased, nan, ex...\n",
       "15732    [source, nestlecom, , , , , firstname, nelson,...\n",
       "15733    [from, lanneytanyachennaibranch, control, , [m...\n",
       "15734    [hello, , , , i, bought, , , gm, pack, of, nes...\n",
       "15735    [consumer, concern, consumer, called, in, for,...\n",
       "15736    [consumer, concern, consumer, called, up, as, ...\n",
       "15737                                                  [n]\n",
       "15738    [to, , nestle, consumer, care, , , , , , respe...\n",
       "15739    [hi, team, , this, is, to, bring, your, kind, ...\n",
       "15740    [we, buy, wheat, apple, to, feed, my, babyhe, ...\n",
       "15741                           [shortage, in, intact, , ]\n",
       "Name: soundBites, Length: 15742, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['soundBites']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the empty strings from the list of tokens\n",
    "df[\"soundBites\"] = df[\"soundBites\"].apply(lambda row: list(filter(None,row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating list of stopwords\n",
    "stopword = set(stopwords.words('english'))\n",
    "stop_list = set(['maggi','maggie','also','nc','rz','njc','like','cz','gonna','carolina','garmin',\n",
    "                 'screenshot','team','wish','provide','screen','rzd','moq','cgof','pcs','kis','ko','rt','customer','called',\n",
    "                'informed','consumer'])\n",
    "stopword.update(stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stopwords from tokens\n",
    "df[\"soundBites\"] = df[\"soundBites\"].apply(lambda row :[item for item in row if item not in stopword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['soundBites']  = df['soundBites'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "df[\"soundBites\"] = df[\"soundBites\"].apply(lambda row: wnl.lemmatize(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#including Hashtags\n",
    "df['soundBites_with_hashtags'] = df['Sound Bite Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([888,889,890],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      ic sent by :- @money_s1ngh  #mechanical #3rdye...\n",
       "1      #foodie#coldcoffee#coffee#nescafe#colddrink#dr...\n",
       "2                              ??nescafe, futala lake ..\n",
       "3      #monsoondiaries#baarishkibaatein#pyaarbharamau...\n",
       "4      49 ?250.00 prime tang lemon instant drink mix,...\n",
       "5      all you need is just a perfect timing ??  rise...\n",
       "6      @cafe_illusions_trimurti has a great collectio...\n",
       "7      what else does one need ??? #netflix #netflixo...\n",
       "8      hey fam ??  so today i have something for u ll...\n",
       "9      ???? #simplybeingfit #strongasbhawna?? #nescaf...\n",
       "10     #indianblogger #coffee #cafe #coffeetime #coff...\n",
       "11     in the nestle-starbucks case, for example, exp...\n",
       "12     yassssss we have a/c & tetley tea & nescafe & ...\n",
       "13     by entering a marketing pact with starbucks, t...\n",
       "14     it comes as nestle s nescafe brand of instant ...\n",
       "15     nestle is taking a page from jab s strategy, a...\n",
       "16     nestle also added niche brand chameleon cold-b...\n",
       "17             location- nescafe illusions, dharampeth .\n",
       "18     the nescafe and nespresso owner would own the ...\n",
       "19     #coffee #nescafe #espresso #latte #cafe #roost...\n",
       "20     #india #bangalore #karnataka #canonphotography...\n",
       "21     may affair: 25.05.2018 #summers #summerfeels #...\n",
       "22     #like4like #follow4follow #instalike #instagoo...\n",
       "23     @nescafe @nescafeindia #nescafegold #nescafe #...\n",
       "24     ?????? ???????????????? #nescafe #coffee #cafe...\n",
       "25     #food #foodie #platter #tasty #tastyfood #tast...\n",
       "26     the agreement gives nestle, which owns the nes...\n",
       "27     nescafe chilled latte ? amazingly refreshing ?...\n",
       "28     remember its going to get worse before it gets...\n",
       "29     s t a r b u c ks  #javachip #redvelvet #starbu...\n",
       "                             ...                        \n",
       "862                          post deleted by the author.\n",
       "863    rt @congressdahan: indians brush colgate shave...\n",
       "864    travel is the only thing that makes you richer...\n",
       "865    #red #abstract #danger #patterns #justamorning...\n",
       "866    midday instant courage . . . #lifeofaphotograp...\n",
       "867    @bloombergquint @menakadoshi not sure what thi...\n",
       "868    #coffee #blackcoffee #nescafe#coffelovers #cof...\n",
       "869    the best tea is made not  from nescafe,  itz m...\n",
       "870           #tea #morningtea #nescafe #chef #chayakada\n",
       "871    ??the coffee saga part 1 nescaf  or turkish co...\n",
       "872    tough decisions?? #coffee experience that rend...\n",
       "873    nescaf  ready-to-drink hazelnut coffee is conv...\n",
       "874                      sip or shoot, it s your coffee \n",
       "875    nescaf  hazelnut brings out a delicious nutty ...\n",
       "876    #nescafe #hazelnut #servechilled #coldcoffee #...\n",
       "877    @brilliantmusicians @indiansingers  @stars.unc...\n",
       "878    monday morning:  palms & pouts ?? #mondaymotiv...\n",
       "879    #lifeofaphotographer #nescafegold #caffeineadd...\n",
       "880    the agreement gives nestle, which owns the nes...\n",
       "881    #nescafe #reading #coffee #fridaynight #howtok...\n",
       "882    rt @spectatorindex: consumers reached, 2017. (...\n",
       "883    rt @friendstv: classic nestle toulouse. pic.tw...\n",
       "884    workship ur work #officespace #nightshift & #n...\n",
       "885    rt @congressdahan: indians brush colgate shave...\n",
       "886    #photography #loveiscamera #cameralove #xoxo #...\n",
       "887    smoky morning blues #nescafe #mornings #motiva...\n",
       "891                                               stream\n",
       "892                                    coffee & beverage\n",
       "893    date range: 4/1/18 12:00 am - 6/30/18 11:59 pm...\n",
       "894           jul 02 2018 05:14:36 am(gmt-5:00) new_york\n",
       "Name: soundBites_with_hashtags, Length: 892, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['soundBites_with_hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing words not containing characters,hashtags or underscores\n",
    "df['soundBites_with_hashtags'] = df[\"soundBites_with_hashtags\"].apply(lambda row: re.sub('[^a-zA-Z0-9#-_]',' ',row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['soundBites_with_hashtags'] = df[\"soundBites_with_hashtags\"].apply(lambda row: re.sub('@[a-zA-Z0-9_]+',' ',row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['soundBites_with_hashtags'] = df[\"soundBites_with_hashtags\"].apply(lambda row: re.sub('[;+>.\\/<,)❤☕₹⛩(&?:!%*=-]+','',row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['soundBites_with_hashtags'] = df[\"soundBites_with_hashtags\"].apply(lambda row: re.sub('_{2,}','',row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"soundBites_with_hashtags\"] = df[\"soundBites_with_hashtags\"].apply(lambda row: re.sub('Â\\x92',\"\\'\",row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"soundBites_with_hashtags\"] = df[\"soundBites_with_hashtags\"].apply(lambda row: re.sub('Â\\x91',\"\\'\",row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"soundBites_with_hashtags\"] = df[\"soundBites_with_hashtags\"].apply(lambda row: re.sub('http\\S','',row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      ic sent by     #mechanical #3rdyear #branch #s...\n",
       "1      #foodie#coldcoffee#coffee#nescafe#colddrink#dr...\n",
       "2                                   nescafe futala lake \n",
       "3      #monsoondiaries#baarishkibaatein#pyaarbharamau...\n",
       "4      49 25000 prime tang lemon instant drink mix 50...\n",
       "5      all you need is just a perfect timing   rise a...\n",
       "6        has a great collection of food and #nescafe ...\n",
       "7      what else does one need  #netflix #netflixorig...\n",
       "8      hey fam   so today i have something for u ll a...\n",
       "9       #simplybeingfit #strongasbhawna #nescafe #lat...\n",
       "10     #indianblogger #coffee #cafe #coffeetime #coff...\n",
       "11     in the nestlestarbucks case for example expert...\n",
       "12     yassssss we have ac  tetley tea  nescafe  a sw...\n",
       "13     by entering a marketing pact with starbucks th...\n",
       "14     it comes as nestle s nescafe brand of instant ...\n",
       "15     nestle is taking a page from jab s strategy as...\n",
       "16     nestle also added niche brand chameleon coldbr...\n",
       "17                location nescafe illusions dharampeth \n",
       "18     the nescafe and nespresso owner would own the ...\n",
       "19     #coffee #nescafe #espresso #latte #cafe #roost...\n",
       "20     #india #bangalore #karnataka #canonphotography...\n",
       "21     may affair 25052018 #summers #summerfeels #may...\n",
       "22     #like4like #follow4follow #instalike #instagoo...\n",
       "23         #nescafegold #nescafe #coffee #coffeeholic...\n",
       "24       #nescafe #coffee #cafe #instacoffee  tag #ta...\n",
       "25     #food #foodie #platter #tasty #tastyfood #tast...\n",
       "26     the agreement gives nestle which owns the nesc...\n",
       "27     nescafe chilled latte  amazingly refreshing   ...\n",
       "28     remember its going to get worse before it gets...\n",
       "29     s t a r b u c ks  #javachip #redvelvet #starbu...\n",
       "                             ...                        \n",
       "862                           post deleted by the author\n",
       "863    rt   indians brush colgate shave gillette bath...\n",
       "864    travel is the only thing that makes you richer...\n",
       "865    #red #abstract #danger #patterns #justamorning...\n",
       "866    midday instant courage    #lifeofaphotographer...\n",
       "867        not sure what this will mean for coffee lo...\n",
       "868    #coffee #blackcoffee #nescafe#coffelovers #cof...\n",
       "869    the best tea is made not  from nescafe  itz ma...\n",
       "870           #tea #morningtea #nescafe #chef #chayakada\n",
       "871    the coffee saga part 1 nescaf  or turkish coffee \n",
       "872    tough decisions #coffee experience that render...\n",
       "873    nescaf  readytodrink hazelnut coffee is conven...\n",
       "874                       sip or shoot it s your coffee \n",
       "875    nescaf  hazelnut brings out a delicious nutty ...\n",
       "876    #nescafe #hazelnut #servechilled #coldcoffee #...\n",
       "877          uncovered       official             #di...\n",
       "878    monday morning  palms  pouts  #mondaymotivatio...\n",
       "879    #lifeofaphotographer #nescafegold #caffeineadd...\n",
       "880    the agreement gives nestle which owns the nesc...\n",
       "881    #nescafe #reading #coffee #fridaynight #howtok...\n",
       "882    rt   consumers reached 2017 billion people coc...\n",
       "883    rt   classic nestle toulouse pictwittercomk6ba...\n",
       "884    workship ur work #officespace #nightshift  #ne...\n",
       "885    rt   indians brush colgate shave gillette bath...\n",
       "886    #photography #loveiscamera #cameralove #xoxo #...\n",
       "887    smoky morning blues #nescafe #mornings #motiva...\n",
       "891                                               stream\n",
       "892                                     coffee  beverage\n",
       "893    date range 4118 1200 am  63018 1159 pm gmt0400...\n",
       "894                 jul 02 2018 051436 amgmt500 new_york\n",
       "Name: soundBites_with_hashtags, Length: 892, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['soundBites_with_hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['soundBites_with_hashtags'] = df[\"soundBites_with_hashtags\"].apply(lambda row: re.sub(r'[.]$',' ',row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing dates and digits\n",
    "df[\"soundBites_with_hashtags\"] = df[\"soundBites_with_hashtags\"].apply(lambda row: re.sub('\\d+','',row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['soundBites_with_hashtags'] = df['soundBites_with_hashtags'].apply(lambda row: row.replace('-Minute','2-Minute'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['soundBites_with_hashtags'] = df['soundBites_with_hashtags'].apply(lambda row: row.replace('-minute','2-Minute'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['soundBites_with_hashtags'] = df['soundBites_with_hashtags'].apply(lambda row: re.split(r'(#\\w+)',row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['soundBites_with_hashtags'] = df['soundBites_with_hashtags'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "df[\"soundBites_with_hashtags\"] = df[\"soundBites_with_hashtags\"].apply(lambda row: wnl.lemmatize(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['soundBites_with_hashtags'] = df['soundBites_with_hashtags'].apply(lambda row: re.split(r' ',row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the empty strings from the list of tokens\n",
    "df[\"soundBites_with_hashtags\"] = df[\"soundBites_with_hashtags\"].apply(lambda row: list(filter(None,row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = set(stopwords.words('english'))\n",
    "stopword_update = [item for item in stop_words.get_stop_words('en') if item not in nltk.corpus.stopwords.words('english')]\n",
    "stop_list = set(['maggi','maggie','#maggi','#maggie','also','njc','nc','rz','use','like','carolina','food','love','garmin','new',\n",
    "                 'raha','pm','tags','pak','across','ki','hi','screenshot','team','wish','provide','screen',\n",
    "                 'pictwittercomclsauwtd',\n",
    "                 '#foodie','#instafood','#foodblogger','tag','rt','updates','reuters','#fooddiaries','#foodisbae','#foodography',\n",
    "                '#likeforlike','#followforfollow','follow','join','rs','#foodlove','delicious','#foodgasm','#foodielife','simpsons'\n",
    "                 '#foodblogger','#merimaggie','#merimaggi','try','#indianblogger','#maggilove','#njcs','#maggielove','#foodporn',\n",
    "                '#food','#foodie','#foodporn','#zomato','#maniamaggi','#tastymaggi','#maniamaggie','#tastymaggie','hai','u','km',\n",
    "                 'tasty','yummy','#yummylicious','#yummilicious','#foddy','#foodgram','#maggilover','#maggielover','#tasty','rda',\n",
    "                 '#yummy','#foddie','#swiggyindia','#zomatoindia','#swiggy','#foodstagram','#instagood','#indianfood','#likelike',\n",
    "                 '#love','#delicious','#foodlover,''#picoftheday'',#noodles','#eeeats','#eeeeeats','#foodpics','#picoftheday',\n",
    "                'noodles','#sodelhi','#likelike','#nomnom','#buzzfoodfeed','#fgram','#eatingforinsta','#saadidilli','kuch',\n",
    "                '#saddidilli','#ll','#flt','#foodphotography','#foodtalkindia','#delhifoodie','#foodlover','#maggilovers',\n",
    "                '#maggielovers','#foodtalkindia','#delhifoodblogger','#delhigram','#instagram','#instagood','#photooftheday',\n",
    "                '#instadaily','#followfollow','#fgrams','#streetfood','#igers','#','#iger','#followme','#foodies','#likes',\n",
    "                 '#instalike','#instalike','#indianfoodbloggers','indianfoodblogger','#foodshot','#foodcoma','gunsmoke','#noodles',\n",
    "                '#vscodelhi','#foodblog','#foodiegram','#foodphoto','#delhi_igers','#delhifood','#delhidiaries','#mumbaifood',\n",
    "                 '#mumbaidiaries','#foodoftheday','#indianfoodie','#foodaholic','#foods','#vscocam','#vscofood','#likeforfollow',\n",
    "                '#delhifoodie','#streetfoods','#eat','#yum','#follow','th','us','#follow','#india','#ahmedabad','#chennai','#delhi',\n",
    "                '#mumbai','#maggiloverforever','#vsco','#likesforlikes','#buzzfeedfood','#delhiblogger','#dfordelhi',\n",
    "                '#tastyfood','#foodiesofinstagram','#knowyourmaggi'])\n",
    "stopword.update(stop_list),\n",
    "stopword.update(set(stopword_update))\n",
    "stopword.update(set(string.ascii_lowercase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stopwords from tokens\n",
    "df[\"soundBites_with_hashtags\"] = df[\"soundBites_with_hashtags\"].apply(lambda row: [item for item in row if item not in stopword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"soundBites_with_hashtags\"] = df[\"soundBites_with_hashtags\"].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      ic sent by #mechanical #rdyear #branch #simple...\n",
       "1      #foodie #coldcoffee #coffee #nescafe #colddrin...\n",
       "2                                    nescafe futala lake\n",
       "3      #monsoondiaries #baarishkibaatein #pyaarbharam...\n",
       "4      prime tang lemon instant drink mix g pouch pri...\n",
       "5      all you need is just a perfect timing rise and...\n",
       "6      has a great collection of food and #nescafe s ...\n",
       "7      what else does one need #netflix #netflixorigi...\n",
       "8      hey fam so today i have something for u ll all...\n",
       "9      #simplybeingfit #strongasbhawna #nescafe #latt...\n",
       "10     #indianblogger #coffee #cafe #coffeetime #coff...\n",
       "11     in the nestlestarbucks case for example expert...\n",
       "12     yassssss we have ac tetley tea nescafe a swimm...\n",
       "13     by entering a marketing pact with starbucks th...\n",
       "14     it comes as nestle s nescafe brand of instant ...\n",
       "15     nestle is taking a page from jab s strategy as...\n",
       "16     nestle also added niche brand chameleon coldbr...\n",
       "17                 location nescafe illusions dharampeth\n",
       "18     the nescafe and nespresso owner would own the ...\n",
       "19     #coffee #nescafe #espresso #latte #cafe #roost...\n",
       "20     #india #bangalore #karnataka #canonphotography...\n",
       "21     may affair #summers #summerfeels #may #mayaffa...\n",
       "22     #likelike #followfollow #instalike #instagood ...\n",
       "23     #nescafegold #nescafe #coffee #coffeeholic #co...\n",
       "24     #nescafe #coffee #cafe #instacoffee tag #tagsf...\n",
       "25     #food #foodie #platter #tasty #tastyfood #tast...\n",
       "26     the agreement gives nestle which owns the nesc...\n",
       "27     nescafe chilled latte amazingly refreshing che...\n",
       "28     remember its going to get worse before it gets...\n",
       "29     s t a r b u c ks #javachip #redvelvet #starbuc...\n",
       "                             ...                        \n",
       "862                           post deleted by the author\n",
       "863    rt indians brush colgate shave gillette bath p...\n",
       "864    travel is the only thing that makes you richer...\n",
       "865    #red #abstract #danger #patterns #justamorning...\n",
       "866    midday instant courage #lifeofaphotographer #n...\n",
       "867    not sure what this will mean for coffee lovers...\n",
       "868    #coffee #blackcoffee #nescafe #coffelovers #co...\n",
       "869    the best tea is made not from nescafe itz made...\n",
       "870           #tea #morningtea #nescafe #chef #chayakada\n",
       "871        the coffee saga part nescaf or turkish coffee\n",
       "872    tough decisions #coffee experience that render...\n",
       "873    nescaf readytodrink hazelnut coffee is conveni...\n",
       "874                        sip or shoot it s your coffee\n",
       "875    nescaf hazelnut brings out a delicious nutty i...\n",
       "876    #nescafe #hazelnut #servechilled #coldcoffee #...\n",
       "877    uncovered official #dilkotumse #rhtdm #oldsong...\n",
       "878    monday morning palms pouts #mondaymotivation c...\n",
       "879    #lifeofaphotographer #nescafegold #caffeineadd...\n",
       "880    the agreement gives nestle which owns the nesc...\n",
       "881    #nescafe #reading #coffee #fridaynight #howtok...\n",
       "882    rt consumers reached billion people cocacola c...\n",
       "883     rt classic nestle toulouse pictwittercomkbakeavl\n",
       "884    workship ur work #officespace #nightshift #nes...\n",
       "885    rt indians brush colgate shave gillette bath p...\n",
       "886    #photography #loveiscamera #cameralove #xoxo #...\n",
       "887    smoky morning blues #nescafe #mornings #motiva...\n",
       "891                                               stream\n",
       "892                                      coffee beverage\n",
       "893                        date range am pm gmt new york\n",
       "894                                   jul amgmt new_york\n",
       "Name: soundBites_with_hashtags, Length: 892, dtype: object"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"soundBites_with_hashtags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('May_Personas_Clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['soundBites_with_hashtags'][9829]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Maggi_Clean2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[df['Sentiments'].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "data.loc[data['Sentiments'] == 'None','Sentiments'] = 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('.\\Desktop\\Stream.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating seperate dataset for Classification\n",
    "dataset = pd.DataFrame(data['soundBites'])\n",
    "dataset['Sentiments'] = data['Sentiments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#https://docs.python.org/2/library/ast.html#ast.literal_eval\n",
    "dataset['soundBites'] = dataset['soundBites'].apply(lambda row: literal_eval(''.join(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a string from list of tokens\n",
    "dataset['soundBites'] = dataset['soundBites'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.copy()\n",
    "dataset.rename(columns = {'Sentiment': 'Sentiments'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting training and testing data\n",
    "x_train,x_test,y_train,y_test = train_test_split(dataset['soundBites'],dataset['Sentiments'], random_state = 20, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying CountVectorizer to get token count\n",
    "cv = CountVectorizer()\n",
    "X_train = cv.fit_transform(x_train)\n",
    "X_test = cv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4630"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train) + len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying TFIDF\n",
    "tfidf = TfidfTransformer()\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8898488120950324"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting sparse training and testing datasets to dense matrices for classification\n",
    "X_train = X_train.todense()\n",
    "X_test = X_test.todense()\n",
    "#Classification using Multinomial Naive Bayes\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train,y_train)\n",
    "prediction = classifier.predict(X_test)\n",
    "accuracy_score(y_true = y_test, y_pred = prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5673146148308136"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classifcation using Gaussian Naive Bayes\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train,y_train)\n",
    "prediction = classifier.predict(X_test)\n",
    "accuracy_score(y_true = y_test, y_pred = prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9006479481641468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 17,  95,   5],\n",
       "       [  9, 871,  19],\n",
       "       [  1,   9, 363]], dtype=int64)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classification Using Decision Tree Classifier\n",
    "clfdt = DecisionTreeClassifier(random_state = 42)\n",
    "clfdt.fit(X_train,y_train)\n",
    "prediction = clfdt.predict(X_test)\n",
    "accuracy_score(y_true = y_test,y_pred = prediction)\n",
    "print(\"Accuracy: \",accuracy_score(y_pred = prediction,y_true = y_test))\n",
    "confusion_matrix(y_true = y_test, y_pred = prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.912167026637869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 16,  97,   4],\n",
       "       [  0, 890,   9],\n",
       "       [  0,  12, 361]], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classification Using Support Vector Classifier\n",
    "clfSVM = LinearSVC()\n",
    "clfSVM.fit(X_train,y_train)\n",
    "prediction = clfSVM.predict(X_test)\n",
    "print(\"Accuracy: \",accuracy_score(y_pred = prediction,y_true = y_test))\n",
    "confusion_matrix(y_true = y_test, y_pred = prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8984881209503239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  9,  98,  10],\n",
       "       [  2, 880,  17],\n",
       "       [  0,  14, 359]], dtype=int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classification Using Random Forest Classifier\n",
    "rfclf = RandomForestClassifier(random_state = 0)\n",
    "rfclf.fit(X_train,y_train)\n",
    "prediction = rfclf.predict(X_test)\n",
    "accuracy_score(y_true = y_test, y_pred = prediction)\n",
    "print(\"Accuracy: \",accuracy_score(y_pred = prediction,y_true = y_test))\n",
    "confusion_matrix(y_true = y_test, y_pred = prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset['Sentiments']=='P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9136069114470843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 20,  96,   1],\n",
       "       [  0, 896,   3],\n",
       "       [  4,  16, 353]], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classification using XGBoost Classifier\n",
    "xgb = XGBClassifier(random_state = 21)\n",
    "xgb.fit(X_train,y_train)\n",
    "prediction = xgb.predict(X_test)\n",
    "accuracy_score(y_true = y_test, y_pred = prediction)\n",
    "print(\"Accuracy: \",accuracy_score(y_pred = prediction,y_true = y_test))\n",
    "confusion_matrix(y_true = y_test, y_pred = prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([pd.DataFrame(X_train),pd.DataFrame(X_test)])\n",
    "Y = pd.concat([y_train,y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6996830427892234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   1,   8],\n",
       "       [  1, 123, 263],\n",
       "       [  3, 103, 760]], dtype=int64)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfdt = DecisionTreeClassifier(random_state = 42)\n",
    "clfdt.fit(X_test,y_test)\n",
    "prediction = clfdt.predict(X_train)\n",
    "accuracy_score(y_true = y_train,y_pred = prediction)\n",
    "print(\"Accuracy: \",accuracy_score(y_pred = prediction,y_true = y_train))\n",
    "confusion_matrix(y_true = y_train, y_pred = prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1262, 4)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelEncoder = LabelEncoder()\n",
    "Y_train = labelEncoder.fit_transform(y_train)\n",
    "Y_test = labelEncoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'Neutral', 'P', 'P0'], dtype=object)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelEncoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = OneHotEncoder()\n",
    "Y_train_ = one_hot_encoder.fit_transform(Y_train.reshape(-1,1))\n",
    "Y_test_ = one_hot_encoder.transform(Y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3241/3241 [==============================] - 2s 563us/step - loss: 0.6463 - acc: 0.7877\n",
      "Epoch 2/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.2941 - acc: 0.9003\n",
      "Epoch 3/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.2481 - acc: 0.9087\n",
      "Epoch 4/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.2226 - acc: 0.9272\n",
      "Epoch 5/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.2074 - acc: 0.9324\n",
      "Epoch 6/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1975 - acc: 0.9352\n",
      "Epoch 7/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1922 - acc: 0.9358\n",
      "Epoch 8/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1888 - acc: 0.9371\n",
      "Epoch 9/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1874 - acc: 0.9367\n",
      "Epoch 10/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1877 - acc: 0.9371\n",
      "Epoch 11/100\n",
      "3241/3241 [==============================] - 0s 135us/step - loss: 0.1870 - acc: 0.9371\n",
      "Epoch 12/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1855 - acc: 0.9371\n",
      "Epoch 13/100\n",
      "3241/3241 [==============================] - 0s 135us/step - loss: 0.1847 - acc: 0.9371\n",
      "Epoch 14/100\n",
      "3241/3241 [==============================] - 0s 149us/step - loss: 0.1850 - acc: 0.9371\n",
      "Epoch 15/100\n",
      "3241/3241 [==============================] - 0s 144us/step - loss: 0.1842 - acc: 0.9371\n",
      "Epoch 16/100\n",
      "3241/3241 [==============================] - 0s 140us/step - loss: 0.1818 - acc: 0.9371\n",
      "Epoch 17/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1835 - acc: 0.9371\n",
      "Epoch 18/100\n",
      "3241/3241 [==============================] - 0s 140us/step - loss: 0.1834 - acc: 0.9371\n",
      "Epoch 19/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1836 - acc: 0.9371\n",
      "Epoch 20/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1834 - acc: 0.9371\n",
      "Epoch 21/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1842 - acc: 0.9371\n",
      "Epoch 22/100\n",
      "3241/3241 [==============================] - 0s 135us/step - loss: 0.1827 - acc: 0.9371\n",
      "Epoch 23/100\n",
      "3241/3241 [==============================] - 0s 135us/step - loss: 0.1831 - acc: 0.9371 0s - loss: 0.1799 - acc: 0.939\n",
      "Epoch 24/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1836 - acc: 0.9371\n",
      "Epoch 25/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1829 - acc: 0.9371\n",
      "Epoch 26/100\n",
      "3241/3241 [==============================] - 0s 135us/step - loss: 0.1824 - acc: 0.9371\n",
      "Epoch 27/100\n",
      "3241/3241 [==============================] - 0s 140us/step - loss: 0.1829 - acc: 0.9371\n",
      "Epoch 28/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1829 - acc: 0.9371\n",
      "Epoch 29/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1820 - acc: 0.9371\n",
      "Epoch 30/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1827 - acc: 0.9371\n",
      "Epoch 31/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1833 - acc: 0.9371\n",
      "Epoch 32/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1826 - acc: 0.9371\n",
      "Epoch 33/100\n",
      "3241/3241 [==============================] - 0s 135us/step - loss: 0.1816 - acc: 0.9371\n",
      "Epoch 34/100\n",
      "3241/3241 [==============================] - 0s 140us/step - loss: 0.1822 - acc: 0.9371\n",
      "Epoch 35/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1821 - acc: 0.9371\n",
      "Epoch 36/100\n",
      "3241/3241 [==============================] - 0s 135us/step - loss: 0.1816 - acc: 0.9371\n",
      "Epoch 37/100\n",
      "3241/3241 [==============================] - 0s 135us/step - loss: 0.1822 - acc: 0.9371\n",
      "Epoch 38/100\n",
      "3241/3241 [==============================] - 0s 140us/step - loss: 0.1823 - acc: 0.9371\n",
      "Epoch 39/100\n",
      "3241/3241 [==============================] - 0s 140us/step - loss: 0.1819 - acc: 0.9371\n",
      "Epoch 40/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1814 - acc: 0.9371\n",
      "Epoch 41/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1819 - acc: 0.9371\n",
      "Epoch 42/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1822 - acc: 0.9371\n",
      "Epoch 43/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1812 - acc: 0.9371\n",
      "Epoch 44/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1820 - acc: 0.9371\n",
      "Epoch 45/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1817 - acc: 0.9371\n",
      "Epoch 46/100\n",
      "3241/3241 [==============================] - 0s 140us/step - loss: 0.1814 - acc: 0.9371\n",
      "Epoch 47/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1816 - acc: 0.9371\n",
      "Epoch 48/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1817 - acc: 0.9371\n",
      "Epoch 49/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1817 - acc: 0.9371\n",
      "Epoch 50/100\n",
      "3241/3241 [==============================] - 0s 140us/step - loss: 0.1817 - acc: 0.9371\n",
      "Epoch 51/100\n",
      "3241/3241 [==============================] - 0s 154us/step - loss: 0.1818 - acc: 0.9371\n",
      "Epoch 52/100\n",
      "3241/3241 [==============================] - 0s 149us/step - loss: 0.1810 - acc: 0.9371\n",
      "Epoch 53/100\n",
      "3241/3241 [==============================] - 0s 144us/step - loss: 0.1818 - acc: 0.9371\n",
      "Epoch 54/100\n",
      "3241/3241 [==============================] - 0s 120us/step - loss: 0.1813 - acc: 0.9371\n",
      "Epoch 55/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1815 - acc: 0.9371\n",
      "Epoch 56/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1813 - acc: 0.9371\n",
      "Epoch 57/100\n",
      "3241/3241 [==============================] - 0s 140us/step - loss: 0.1821 - acc: 0.9371\n",
      "Epoch 58/100\n",
      "3241/3241 [==============================] - 0s 140us/step - loss: 0.1814 - acc: 0.9371\n",
      "Epoch 59/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1813 - acc: 0.9371\n",
      "Epoch 60/100\n",
      "3241/3241 [==============================] - 0s 120us/step - loss: 0.1813 - acc: 0.9371\n",
      "Epoch 61/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1818 - acc: 0.9371\n",
      "Epoch 62/100\n",
      "3241/3241 [==============================] - 0s 140us/step - loss: 0.1812 - acc: 0.9371\n",
      "Epoch 63/100\n",
      "3241/3241 [==============================] - 0s 135us/step - loss: 0.1814 - acc: 0.9371\n",
      "Epoch 64/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1814 - acc: 0.9371\n",
      "Epoch 65/100\n",
      "3241/3241 [==============================] - 0s 120us/step - loss: 0.1811 - acc: 0.9371\n",
      "Epoch 66/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1816 - acc: 0.9371\n",
      "Epoch 67/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1814 - acc: 0.9371\n",
      "Epoch 68/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1812 - acc: 0.9371\n",
      "Epoch 69/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1818 - acc: 0.9371\n",
      "Epoch 70/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1811 - acc: 0.9371\n",
      "Epoch 71/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1816 - acc: 0.9371\n",
      "Epoch 72/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1812 - acc: 0.9371\n",
      "Epoch 73/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1815 - acc: 0.9371\n",
      "Epoch 74/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1810 - acc: 0.9371\n",
      "Epoch 75/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1814 - acc: 0.9371\n",
      "Epoch 76/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1817 - acc: 0.9371\n",
      "Epoch 77/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1811 - acc: 0.9371\n",
      "Epoch 78/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1811 - acc: 0.9371\n",
      "Epoch 79/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1814 - acc: 0.9371\n",
      "Epoch 80/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1816 - acc: 0.9371\n",
      "Epoch 81/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1799 - acc: 0.9371\n",
      "Epoch 82/100\n",
      "3241/3241 [==============================] - 0s 120us/step - loss: 0.1817 - acc: 0.9371\n",
      "Epoch 83/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1814 - acc: 0.9371\n",
      "Epoch 84/100\n",
      "3241/3241 [==============================] - 0s 120us/step - loss: 0.1810 - acc: 0.9371\n",
      "Epoch 85/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1810 - acc: 0.9371\n",
      "Epoch 86/100\n",
      "3241/3241 [==============================] - 0s 120us/step - loss: 0.1811 - acc: 0.9371\n",
      "Epoch 87/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1811 - acc: 0.9371\n",
      "Epoch 88/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1813 - acc: 0.9371\n",
      "Epoch 89/100\n",
      "3241/3241 [==============================] - 0s 144us/step - loss: 0.1811 - acc: 0.9371\n",
      "Epoch 90/100\n",
      "3241/3241 [==============================] - 0s 140us/step - loss: 0.1814 - acc: 0.9371\n",
      "Epoch 91/100\n",
      "3241/3241 [==============================] - 0s 140us/step - loss: 0.1812 - acc: 0.9371\n",
      "Epoch 92/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1812 - acc: 0.9371\n",
      "Epoch 93/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1813 - acc: 0.9371\n",
      "Epoch 94/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1810 - acc: 0.9371\n",
      "Epoch 95/100\n",
      "3241/3241 [==============================] - 0s 130us/step - loss: 0.1813 - acc: 0.9371\n",
      "Epoch 96/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1813 - acc: 0.9371\n",
      "Epoch 97/100\n",
      "3241/3241 [==============================] - 0s 120us/step - loss: 0.1813 - acc: 0.9371\n",
      "Epoch 98/100\n",
      "3241/3241 [==============================] - 0s 120us/step - loss: 0.1809 - acc: 0.9371\n",
      "Epoch 99/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1809 - acc: 0.9371\n",
      "Epoch 100/100\n",
      "3241/3241 [==============================] - 0s 125us/step - loss: 0.1814 - acc: 0.9371\n"
     ]
    }
   ],
   "source": [
    "# Using Deeplearning for Classification\n",
    "seed(0)\n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim =  32, input_shape = (X_train.shape[1],),activation = 'relu'))\n",
    "model.add(Dense(output_dim = 32, activation = 'relu'))\n",
    "model.add(Dense(output_dim = 32, activation = 'relu'))\n",
    "model.add(Dense(output_dim = 32, activation = 'relu'))\n",
    "model.add(Dense(output_dim = 4, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adagrad', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(X_train, Y_train_, batch_size = 100, epochs = 100)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.9092872570194385\n",
      "[[ 15  96   6]\n",
      " [  3 886  10]\n",
      " [  2   9 362]]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy = ',accuracy_score(y_true = Y_test, y_pred = y_pred.argmax(axis = 1)))\n",
    "print(confusion_matrix(y_true = Y_test, y_pred = y_pred.argmax(axis = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Maggi_Clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data['Sound Bite Text']:   \n",
    "    i = data.index.values[data['Sound Bite Text'] == item]\n",
    "    scores = analyzer.polarity_scores(item)\n",
    "    if (scores['compound'] < 0.05) & (scores['compound'] > -0.05):\n",
    "        data.loc[i,'vader_Senti'] = 'None'\n",
    "    elif scores['compound'] < -0.05:\n",
    "        data.loc[i,'vader_Senti'] = 'N'\n",
    "    elif scores['compound'] > 0.05:\n",
    "        data.loc[i,'vader_Senti'] = 'P'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Sentiment'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "vader_in = le.fit_transform(data['Sentiment'])\n",
    "vader_out = le.transform(data['vader_Senti'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['vader_Senti'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true = vader_out, y_pred  = vader_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_true = vader_out,y_pred = vader_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['vader_Senti'] == 'N','Sound Bite Text'][42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.loc[df['Sentiment'].dropna(axis = 0).index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['soundBites']\n",
    "Y_train = train['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.loc[df['Sentiment'].isna() == True]\n",
    "X_test = df['soundBites']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X_train = cv.fit_transform(X_train)\n",
    "X_test = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.todense()\n",
    "X_test = X_test.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train,Y_train)\n",
    "df['Decision Tree'] = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification using XGBoost Classifier\n",
    "xgb = XGBClassifier(random_state = 21,n_jobs = cores)\n",
    "xgb.fit(X_train,Y_train)\n",
    "df['XGB'] = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Using Support Vector Classifier\n",
    "clfSVM = LinearSVC()\n",
    "clfSVM.fit(X_train,Y_train)\n",
    "df['SVM'] = clfSVM.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sound Bite Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>soundBites</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>SVM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For AMARPATAN region of Madhya Pradesh, we wis...</td>\n",
       "      <td>P</td>\n",
       "      <td>amarpatan region madhya pradesh nestle product...</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Consumer called up and asked about if he can g...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>asked give nestogen months baby please consult</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Consumer called up as he has a pack of NESTLÃ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>pack nestl cerelac wheat apple cherry wanted k...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Retailer called in again and said that his con...</td>\n",
       "      <td>N</td>\n",
       "      <td>retailer said concern registered previously go...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Consumer called to inquire about the method of...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>inquire method preparation nan excella pro mon...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Consumer called up as he wanted to know about ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>wanted know expiration date minute noodles gms...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Consumer called us and shared the feedback for...</td>\n",
       "      <td>P</td>\n",
       "      <td>us shared feedback nestle products happy nestle</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Consumer called in as she is using HIGH PROTEI...</td>\n",
       "      <td>N</td>\n",
       "      <td>using high protein resource however available ...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SOURCE: nestle.com \\r\\n\\r\\nFIRSTNAME: Sharad\\r...</td>\n",
       "      <td>P</td>\n",
       "      <td>source nestlecom firstname sharad lastname tan...</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Customer is looking for a technician for Two v...</td>\n",
       "      <td>N</td>\n",
       "      <td>looking technician two vending machine showing...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Consumer called to inquire about the shelf lif...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>inquire shelf life minute noodle information s...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SOURCE: nestle.com \\r\\n\\r\\nFIRSTNAME: Jay\\r\\n\\...</td>\n",
       "      <td>P</td>\n",
       "      <td>source nestlecom firstname jay lastname dey em...</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Consumer is looking for Technician for NNS Ven...</td>\n",
       "      <td>N</td>\n",
       "      <td>looking technician nns vending machine complet...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hi Gudevening.,\\r\\nIam Nishan Karkera here. At...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>hi gudevening iam nishan karkera present iam w...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Customer called up as he want distributorship ...</td>\n",
       "      <td>P</td>\n",
       "      <td>want distributorship area forward details conc...</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Consumer is looking for a Technician as she sa...</td>\n",
       "      <td>N</td>\n",
       "      <td>looking technician said still noise issue nns ...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Consumer called saying that he is feeding LACT...</td>\n",
       "      <td>N</td>\n",
       "      <td>saying feeding lactogen month baby growth foll...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>*Dear Sir,*\\r\\n\\r\\n*I am  D.Durai Murugan   ? ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>dear sir ddurai murugan manufacturing excellen...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Consumer called in to inquire about the availa...</td>\n",
       "      <td>N</td>\n",
       "      <td>inquire availability nestle resource high prot...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Consumer Concern : Consumer called and inquire...</td>\n",
       "      <td>N</td>\n",
       "      <td>concern inquired manufacturing expiring date m...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sub:- Resume For Requirement\\r\\n\\r\\n*Dear Sir,...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>sub resume requirement dear sir ddurai murugan...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>We are not receiving the supply of Nestle item...</td>\n",
       "      <td>N</td>\n",
       "      <td>receiving supply nestle item cerelac chocolate...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Kindly change your latest Maggi TV Commercial....</td>\n",
       "      <td>N</td>\n",
       "      <td>kindly change latest tv commercial boring bakw...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sub:- Regarding Getting Distributorship or Who...</td>\n",
       "      <td>P</td>\n",
       "      <td>sub regarding getting distributorship wholesal...</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Customer called us and said that he wants dist...</td>\n",
       "      <td>P</td>\n",
       "      <td>us said wants distributorship rajasthan forwar...</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Dear sir/madam ,\\r\\n\\r\\nCame to know about you...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>dear sirmadam came know new puppy food searchi...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sub:- Availability of Maggi\\r\\n\\r\\nDear Sir/Ma...</td>\n",
       "      <td>P</td>\n",
       "      <td>sub availability dear sirmadam i've huge fan n...</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Consumer called and inquired if NESTLE MANUFAC...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>inquired nestle manufactures biscuits manufacture</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>unproductive call</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>unproductive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Consumer called up as he wanted to know about ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>wanted know expiration date lactogen baby mont...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15712</th>\n",
       "      <td>@NestleIndia bought this today at chennai, and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bought today chennai chocolate much portion av...</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15713</th>\n",
       "      <td>Hi Nestle Team\\r\\nI am your regular customer w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hi nestle regular consumes nestle dahi daily t...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15714</th>\n",
       "      <td>My daughter bought me a jar of  Nescafe Classi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>daughter bought jar nescafe classic opened fou...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15715</th>\n",
       "      <td>Consumer Concern: Consumer called in and was s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>concern saying getting display money want file...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15716</th>\n",
       "      <td>Consumer Concern: Called to consumer ,and ment...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>concern mentioned comfortable tamil mentioned ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15717</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>n</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15718</th>\n",
       "      <td>Wat the hell is this put some care to make thi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wat hell put care make product please take car...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15719</th>\n",
       "      <td>This is to bring to your notice about the poor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bring notice poor quality packing nestle class...</td>\n",
       "      <td>P</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15720</th>\n",
       "      <td>- Consumer Concern: Consumer called up and sai...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>concern said purchased packs minute g said ope...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15721</th>\n",
       "      <td>Hai\\r\\n\\r\\nPlease find the attached images of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hai please find attached images uneven chocola...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15722</th>\n",
       "      <td>Consumer Concern: Retailer called ,and he ment...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>concern retailer mentioned facing problem last...</td>\n",
       "      <td>N</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15723</th>\n",
       "      <td>81160455MA April/2018/B 01:38\\r\\nW10 81180452C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>april b w cb april g april b april b member fa...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15724</th>\n",
       "      <td>Dear Nestle India ,\\r\\n\\r\\nI purchased a Quali...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dear nestle india purchased quality streets bo...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15725</th>\n",
       "      <td>CONSUMER'S CONCERN:- Consumer called in with a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>consumer's concern complaint said purchased pa...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15726</th>\n",
       "      <td>Dear Nestle Team,\\r\\n\\r\\nOne of our customer h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dear nestle one purchased nestle hot heads chi...</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15727</th>\n",
       "      <td>Dear,\\r\\n         We are a regular customer of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dear regular nan pro past monthsbut afraid ur ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15728</th>\n",
       "      <td>Consumer Concern: Consumer complained for KITK...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>concern complained kitkat said bought kitkat w...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15729</th>\n",
       "      <td>We have received and delivered  a full case of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>received delivered full case slim milk mention...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15730</th>\n",
       "      <td>Consumer Concern: Consumer called in and state...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>concern stated sensed detergent smell pack sai...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15731</th>\n",
       "      <td>Hi,\\r\\n\\r\\n\\r\\nRecently I purchased Nan excell...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hi recently purchased nan excella pro one medp...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15732</th>\n",
       "      <td>SOURCE: nestle.com \\r\\n\\r\\nFIRSTNAME: NELSON\\r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>source nestlecom firstname nelson lastname jos...</td>\n",
       "      <td>N</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15733</th>\n",
       "      <td>From: Lanney,Tanya,CHENNAI,BRANCH CONTROL\\r\\n[...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lanneytanyachennaibranch control [mailtotanyal...</td>\n",
       "      <td>P</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15734</th>\n",
       "      <td>Hello,\\r\\n\\r\\nI bought 100 gm pack of nescafe ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>hello bought gm pack nescafe classic reliance ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15735</th>\n",
       "      <td>Consumer Concern: Consumer called in for the c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>concern concern asked expiration date till lon...</td>\n",
       "      <td>N</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15736</th>\n",
       "      <td>Consumer Concern: Consumer called up as he wen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>concern went mart shopping observed people nes...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15737</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>n</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15738</th>\n",
       "      <td>To\\r\\nNestle consumer care\\r\\n\\r\\n\\r\\nRespecte...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nestle care respected sir bought product nestl...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15739</th>\n",
       "      <td>Hi team\\r\\nThis is to bring your kind consider...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>hi bring kind consideration yesterday night pr...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15740</th>\n",
       "      <td>We buy wheat apple to feed my baby.he likes th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>buy wheat apple feed babyhe likes flavour skin...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15741</th>\n",
       "      <td>Shortage in Intact Carton</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shortage intact</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15742 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Sound Bite Text Sentiment  \\\n",
       "0      For AMARPATAN region of Madhya Pradesh, we wis...         P   \n",
       "1      Consumer called up and asked about if he can g...   Neutral   \n",
       "2      Consumer called up as he has a pack of NESTLÃ...   Neutral   \n",
       "3      Retailer called in again and said that his con...         N   \n",
       "4      Consumer called to inquire about the method of...   Neutral   \n",
       "5      Consumer called up as he wanted to know about ...   Neutral   \n",
       "6      Consumer called us and shared the feedback for...         P   \n",
       "7      Consumer called in as she is using HIGH PROTEI...         N   \n",
       "8      SOURCE: nestle.com \\r\\n\\r\\nFIRSTNAME: Sharad\\r...         P   \n",
       "9      Customer is looking for a technician for Two v...         N   \n",
       "10     Consumer called to inquire about the shelf lif...   Neutral   \n",
       "11     SOURCE: nestle.com \\r\\n\\r\\nFIRSTNAME: Jay\\r\\n\\...         P   \n",
       "12     Consumer is looking for Technician for NNS Ven...         N   \n",
       "13     Hi Gudevening.,\\r\\nIam Nishan Karkera here. At...   Neutral   \n",
       "14     Customer called up as he want distributorship ...         P   \n",
       "15     Consumer is looking for a Technician as she sa...         N   \n",
       "16     Consumer called saying that he is feeding LACT...         N   \n",
       "17     *Dear Sir,*\\r\\n\\r\\n*I am  D.Durai Murugan   ? ...   Neutral   \n",
       "18     Consumer called in to inquire about the availa...         N   \n",
       "19     Consumer Concern : Consumer called and inquire...         N   \n",
       "20     Sub:- Resume For Requirement\\r\\n\\r\\n*Dear Sir,...   Neutral   \n",
       "21     We are not receiving the supply of Nestle item...         N   \n",
       "22     Kindly change your latest Maggi TV Commercial....         N   \n",
       "23     Sub:- Regarding Getting Distributorship or Who...         P   \n",
       "24     Customer called us and said that he wants dist...         P   \n",
       "25     Dear sir/madam ,\\r\\n\\r\\nCame to know about you...   Neutral   \n",
       "26     Sub:- Availability of Maggi\\r\\n\\r\\nDear Sir/Ma...         P   \n",
       "27     Consumer called and inquired if NESTLE MANUFAC...   Neutral   \n",
       "28                                     unproductive call   Neutral   \n",
       "29     Consumer called up as he wanted to know about ...   Neutral   \n",
       "...                                                  ...       ...   \n",
       "15712  @NestleIndia bought this today at chennai, and...       NaN   \n",
       "15713  Hi Nestle Team\\r\\nI am your regular customer w...       NaN   \n",
       "15714  My daughter bought me a jar of  Nescafe Classi...       NaN   \n",
       "15715  Consumer Concern: Consumer called in and was s...       NaN   \n",
       "15716  Consumer Concern: Called to consumer ,and ment...   Neutral   \n",
       "15717                                                  N         N   \n",
       "15718  Wat the hell is this put some care to make thi...       NaN   \n",
       "15719  This is to bring to your notice about the poor...       NaN   \n",
       "15720  - Consumer Concern: Consumer called up and sai...       NaN   \n",
       "15721  Hai\\r\\n\\r\\nPlease find the attached images of ...       NaN   \n",
       "15722  Consumer Concern: Retailer called ,and he ment...       NaN   \n",
       "15723  81160455MA April/2018/B 01:38\\r\\nW10 81180452C...       NaN   \n",
       "15724  Dear Nestle India ,\\r\\n\\r\\nI purchased a Quali...       NaN   \n",
       "15725  CONSUMER'S CONCERN:- Consumer called in with a...       NaN   \n",
       "15726  Dear Nestle Team,\\r\\n\\r\\nOne of our customer h...       NaN   \n",
       "15727  Dear,\\r\\n         We are a regular customer of...       NaN   \n",
       "15728  Consumer Concern: Consumer complained for KITK...       NaN   \n",
       "15729  We have received and delivered  a full case of...       NaN   \n",
       "15730  Consumer Concern: Consumer called in and state...   Neutral   \n",
       "15731  Hi,\\r\\n\\r\\n\\r\\nRecently I purchased Nan excell...       NaN   \n",
       "15732  SOURCE: nestle.com \\r\\n\\r\\nFIRSTNAME: NELSON\\r...       NaN   \n",
       "15733  From: Lanney,Tanya,CHENNAI,BRANCH CONTROL\\r\\n[...       NaN   \n",
       "15734  Hello,\\r\\n\\r\\nI bought 100 gm pack of nescafe ...   Neutral   \n",
       "15735  Consumer Concern: Consumer called in for the c...       NaN   \n",
       "15736  Consumer Concern: Consumer called up as he wen...       NaN   \n",
       "15737                                                  N         N   \n",
       "15738  To\\r\\nNestle consumer care\\r\\n\\r\\n\\r\\nRespecte...       NaN   \n",
       "15739  Hi team\\r\\nThis is to bring your kind consider...   Neutral   \n",
       "15740  We buy wheat apple to feed my baby.he likes th...       NaN   \n",
       "15741                          Shortage in Intact Carton       NaN   \n",
       "\n",
       "                                              soundBites Decision Tree  \\\n",
       "0      amarpatan region madhya pradesh nestle product...             P   \n",
       "1         asked give nestogen months baby please consult       Neutral   \n",
       "2      pack nestl cerelac wheat apple cherry wanted k...       Neutral   \n",
       "3      retailer said concern registered previously go...             N   \n",
       "4      inquire method preparation nan excella pro mon...       Neutral   \n",
       "5      wanted know expiration date minute noodles gms...       Neutral   \n",
       "6        us shared feedback nestle products happy nestle             P   \n",
       "7      using high protein resource however available ...             N   \n",
       "8      source nestlecom firstname sharad lastname tan...             P   \n",
       "9      looking technician two vending machine showing...             N   \n",
       "10     inquire shelf life minute noodle information s...       Neutral   \n",
       "11     source nestlecom firstname jay lastname dey em...             P   \n",
       "12     looking technician nns vending machine complet...             N   \n",
       "13     hi gudevening iam nishan karkera present iam w...       Neutral   \n",
       "14     want distributorship area forward details conc...             P   \n",
       "15     looking technician said still noise issue nns ...             N   \n",
       "16     saying feeding lactogen month baby growth foll...             N   \n",
       "17     dear sir ddurai murugan manufacturing excellen...       Neutral   \n",
       "18     inquire availability nestle resource high prot...             N   \n",
       "19     concern inquired manufacturing expiring date m...             N   \n",
       "20     sub resume requirement dear sir ddurai murugan...       Neutral   \n",
       "21     receiving supply nestle item cerelac chocolate...             N   \n",
       "22     kindly change latest tv commercial boring bakw...             N   \n",
       "23     sub regarding getting distributorship wholesal...             P   \n",
       "24     us said wants distributorship rajasthan forwar...             P   \n",
       "25     dear sirmadam came know new puppy food searchi...       Neutral   \n",
       "26     sub availability dear sirmadam i've huge fan n...             P   \n",
       "27     inquired nestle manufactures biscuits manufacture       Neutral   \n",
       "28                                          unproductive       Neutral   \n",
       "29     wanted know expiration date lactogen baby mont...       Neutral   \n",
       "...                                                  ...           ...   \n",
       "15712  bought today chennai chocolate much portion av...             P   \n",
       "15713  hi nestle regular consumes nestle dahi daily t...       Neutral   \n",
       "15714  daughter bought jar nescafe classic opened fou...       Neutral   \n",
       "15715  concern saying getting display money want file...             N   \n",
       "15716  concern mentioned comfortable tamil mentioned ...       Neutral   \n",
       "15717                                                  n       Neutral   \n",
       "15718  wat hell put care make product please take car...       Neutral   \n",
       "15719  bring notice poor quality packing nestle class...             P   \n",
       "15720  concern said purchased packs minute g said ope...       Neutral   \n",
       "15721  hai please find attached images uneven chocola...       Neutral   \n",
       "15722  concern retailer mentioned facing problem last...             N   \n",
       "15723  april b w cb april g april b april b member fa...       Neutral   \n",
       "15724  dear nestle india purchased quality streets bo...       Neutral   \n",
       "15725  consumer's concern complaint said purchased pa...             N   \n",
       "15726  dear nestle one purchased nestle hot heads chi...             P   \n",
       "15727  dear regular nan pro past monthsbut afraid ur ...       Neutral   \n",
       "15728  concern complained kitkat said bought kitkat w...       Neutral   \n",
       "15729  received delivered full case slim milk mention...       Neutral   \n",
       "15730  concern stated sensed detergent smell pack sai...       Neutral   \n",
       "15731  hi recently purchased nan excella pro one medp...       Neutral   \n",
       "15732  source nestlecom firstname nelson lastname jos...             N   \n",
       "15733  lanneytanyachennaibranch control [mailtotanyal...             P   \n",
       "15734  hello bought gm pack nescafe classic reliance ...       Neutral   \n",
       "15735  concern concern asked expiration date till lon...             N   \n",
       "15736  concern went mart shopping observed people nes...             N   \n",
       "15737                                                  n       Neutral   \n",
       "15738  nestle care respected sir bought product nestl...       Neutral   \n",
       "15739  hi bring kind consideration yesterday night pr...       Neutral   \n",
       "15740  buy wheat apple feed babyhe likes flavour skin...       Neutral   \n",
       "15741                                    shortage intact       Neutral   \n",
       "\n",
       "           SVM  \n",
       "0            P  \n",
       "1      Neutral  \n",
       "2      Neutral  \n",
       "3            N  \n",
       "4      Neutral  \n",
       "5      Neutral  \n",
       "6            P  \n",
       "7            N  \n",
       "8            P  \n",
       "9            N  \n",
       "10     Neutral  \n",
       "11           P  \n",
       "12           N  \n",
       "13     Neutral  \n",
       "14           P  \n",
       "15           N  \n",
       "16           N  \n",
       "17     Neutral  \n",
       "18           N  \n",
       "19           N  \n",
       "20     Neutral  \n",
       "21           N  \n",
       "22           N  \n",
       "23           P  \n",
       "24           P  \n",
       "25     Neutral  \n",
       "26           P  \n",
       "27     Neutral  \n",
       "28     Neutral  \n",
       "29     Neutral  \n",
       "...        ...  \n",
       "15712        P  \n",
       "15713  Neutral  \n",
       "15714  Neutral  \n",
       "15715        N  \n",
       "15716  Neutral  \n",
       "15717  Neutral  \n",
       "15718  Neutral  \n",
       "15719  Neutral  \n",
       "15720  Neutral  \n",
       "15721  Neutral  \n",
       "15722        P  \n",
       "15723  Neutral  \n",
       "15724  Neutral  \n",
       "15725        N  \n",
       "15726        P  \n",
       "15727  Neutral  \n",
       "15728  Neutral  \n",
       "15729        P  \n",
       "15730  Neutral  \n",
       "15731  Neutral  \n",
       "15732  Neutral  \n",
       "15733  Neutral  \n",
       "15734  Neutral  \n",
       "15735  Neutral  \n",
       "15736        N  \n",
       "15737  Neutral  \n",
       "15738  Neutral  \n",
       "15739  Neutral  \n",
       "15740  Neutral  \n",
       "15741  Neutral  \n",
       "\n",
       "[15742 rows x 5 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7612120442129335"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true = df['SVM'],y_pred = df['Decision Tree'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
